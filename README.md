<<<<<<< HEAD
# FIL-ROUGE
Data engineering

##__Procédé :__ 
* Recuperation des accreditations auprès de Twitter 
* Recupération des données 
* Stockage des données 
* Nettoyage des données 
* Analyse des  données
* Mise en place du modele 
* Test

### I- Pour recuperer les données, nous avons décidé de nous connecter à l'API Twitter via le bus de message Apache Kafka. 


* Mise en place du bus : aka Kafka

Nous avons besoin d'un gestionnaire de brokers. Et comme mentionné dans la documentation, nous nous sommes tournés 
vers Apache Zookeeper. Les commandes suivantes permettent ainsi de : 

Lancement d'un serveur Zookeeper : 
    
   *   ./zkServer.sh start /home/akoffi/Bureau/Tools_fil_rouge/apache-zookeeper-3.5.6-bin/conf/zookeeper.properties
   * __Sous DOS :__ Juste lancer la commande zkServer.cmd 
   * Vous devriez voir la mention __Server started__
   
Lancement d'un serveur kafka : 
   
   * ./kafka-server-start.sh /home/akoffi/Bureau/Tools_fil_rouge/kafka_2.12-2.3.0/config/server.properties
   * kafka-server-start.bat  D:/MS/tools/kafka_2.12-2.3.1/config/server.properties 

TroubleShooting : 

   * Lors du lancement du serveur Kafka, il peut arriver qu'il n'arrive pas à recuperer les logs. 
    Dans ce cas de figure, il convient de modifier le fichier de conf (server.properties), en particulier la ligne relative 
    au repertoire de log ( log_dir). Il suffit de renommer ce fichier avant de relancer le serveur Kafka 

Creation d'un Topic : 

   * ./kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 3 --topic sample_test
   * Le nombre de replica correspond à notre nombre de brokers. 

Instancier une fenetre pour le consumer : 
   * kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic Twitter --from-beginning

Pour lancer la recuperation de la data : 

   * Demarrer le serveur Zookeeper 
   * Demarrer un serveur kafka 
   * Lancer un consumer kafka pour visualiser les données ( optionel)
   * Lancer l'application java 

Limitations :

   * The limits are on number of keywords / user ids etc you can track - these are in the docs.
     
     The limit on tweets received, is 1% of the firehose, which is variable. If the volume of tweets is less than 1% of all tweets posted, you will get all tweets matching. Once you start missing tweets you will start receiving limit notices in the stream.
     
     Depending on what you’re tracking, you may not get any tweets for a while, instead, blank lines are sent to keep the connection alive. You should aim to keep a stable, open connection and not reconnect frequently - however, if no activity or an error occurs you should reconnect, but with exponential backoff (exponentially increasing the delay between reconnect attempts)

Kafka_cheat_sheet : https://ronnieroller.com/kafka/cheat-sheet#listing-messages-from-a-topic

### II- Couche persistance 

Une fois la data collectée, on l'écrit dans une base mongo pour faire un traitement par batch. 

* Pour la consommation des instances dans le bus on passe par la commande : 
C:\Users\koffi\AppData\Local\Programs\Python\Python36\python.exe consummer.py


### III- Entrainement d'un modele d'analyse de sentiment sur un dataset existant


### IV- Prediction temps réel

D:\MS\Spark\spark\bin>

spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2 Streaming.py localhost:2181 Twitter

 

||||||| empty tree
=======
# FIL-ROUGE
Data engineering
Idea 1 : Social network

      Aka Social Data Acquisition and Social Graph Processing

      Today, social networks are the first choice for marketing campaigns. They promise to serve well targeted, viral, highly customizable       advertisement while getting direct customer feedback and engagement. The numbers generated by the Internet companies serving digital       advertisements are astronomical: Google $43B (2012), Facebook $6B, etc…

      In this context of online marketing through social networks, the tasks of this project are split in two parts, the first being more       pragmatic (hands-on) and the second more theoretical:

      Social Data Acquisition
      Crawl a number of public APIs (Facebook, G+, Twitter, etc…)
      Store the data in a database
      Build a web interface to search through the data
      Social Graph Processing
      Starting from one given node in the graph (i.e. a particular company), the social graph will be quantified and analyzed.
      The different data sources (Facebook, Twitter, etc…) will be correlated to discover non obvious links (like sister entities, etc…)         and interactions
      For instance A always retweet B which always retweet C; A, B and C are part of the same cluster
      The goal of the project is to quantify and classify the marketing footprint of companies on social networks.

      The student choosing this project will have the opportunity to acquire in-depth hands-on experience on state-of-the-art APIs,             storage (graph or NoSQL database), graph processing and data mining.
      
Idea 2 : 
      You have StackOverflow data available online (I would recomment exporting it from Google Platform), you can try to derive some             statistics form it (like average score per comment or something more fun, up to you), implement it either with different framework         or     with Spark but with different api (DataFrames API, SQL, DataSets, RDD, etc) and deploy it on any PAAS that you think is             popular (the       major 3 are AWS, Google and Azure). This will give you enough work to actually understand how it all works and it       will help you to         hold a reasonable discussion during interviews.

Idea 3 : 

https://www.kaggle.com/c/predict-who-is-more-influential-in-a-social-network/overview/evaluation

idea 4 : NCAA / PLAYOFFS ,
      Another year, another chance to anticipate the upsets, call the probabilities, and put your bracketology skills to the leaderboard       test. https://www.kaggle.com/c/mens-machine-learning-competition-2019
https://www.kaggle.com/c/bigquery-geotab-intersection-congestion/leaderboard
>>>>>>> 8d8be548f0f274c3d7976161bdd79e94599915fd
